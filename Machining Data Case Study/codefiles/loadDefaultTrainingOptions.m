% training options for pre-training and fine-tuning
function options = loadDefaultTrainingOptions(tag,valdata)

epochs = 100;
if strcmp(tag,"ft")
    miniBatchSize = 32;
elseif strcmp(tag,"pt")
    miniBatchSize = 64;
    epochs = 100;
else
    miniBatchSize = 16;
    fprintf('Using default minibatch size of %d \n',miniBatchSize);
    epochs = 50;
end

% parameters used
valfreq = 20;
ilr = 2e-4;
learnDropFactor = 0.9;
learnRateDropPeriod = valfreq;
globalL2Reg = 2e-3;
gradientDecayFactor = 0.95;
gt = 10;

% if user specifies validation data
if ~isempty(valdata) && ~strcmp(valdata,'none')
    options = trainingOptions(...
        'adam','MiniBatchSize',miniBatchSize,...
        'MaxEpochs',epochs,...
        'InitialLearnRate',ilr,...
        'ValidationData',valdata,...
        'ValidationFrequency',valfreq,...
        'LearnRateDropFactor',learnDropFactor,...
        'LearnRateDropPeriod',learnRateDropPeriod,...
        'L2Regularization',globalL2Reg,...
        'GradientDecayFactor',gradientDecayFactor,...
        'ExecutionEnvironment','auto',...
        'shuffle','every-epoch',...
        'plots','training-progress','Verbose',true);
% otherwise
else
    options = trainingOptions(...
        'adam','MiniBatchSize',miniBatchSize,...
        'MaxEpochs',epochs,...
        'InitialLearnRate',ilr,...
        'LearnRateDropFactor',learnDropFactor,...
        'LearnRateDropPeriod',learnRateDropPeriod,...
        'L2Regularization',globalL2Reg,...
        'GradientDecayFactor',gradientDecayFactor,...
        'ExecutionEnvironment','auto',...
        'shuffle','every-epoch',...
        'plots','training-progress','Verbose',true);    
end
end